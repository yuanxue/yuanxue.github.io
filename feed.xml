<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://yuanxue.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://yuanxue.github.io/" rel="alternate" type="text/html" /><updated>2025-10-19T01:04:32-07:00</updated><id>https://yuanxue.github.io/feed.xml</id><title type="html">Yuan (Emily) Xue</title><subtitle>(last updated: Oct 2025)</subtitle><entry><title type="html">Reflections on Richard Sutton’s Interview: Part I — World Model and Understanding</title><link href="https://yuanxue.github.io/2025/10/06/reflection-sutton-part1.html" rel="alternate" type="text/html" title="Reflections on Richard Sutton’s Interview: Part I — World Model and Understanding" /><published>2025-10-06T00:00:00-07:00</published><updated>2025-10-06T00:00:00-07:00</updated><id>https://yuanxue.github.io/2025/10/06/reflection-sutton-part1</id><content type="html" xml:base="https://yuanxue.github.io/2025/10/06/reflection-sutton-part1.html"><![CDATA[<h3 id="suttons-view-lack-of-world-model-in-llms">Sutton’s View: Lack of World Model in LLMs</h3>

<p>Sutton begins with a simple observation: <strong>LLMs never experience the world.</strong> He argues that these systems <strong>lack a world model</strong> — a sense of cause and effect, surprise, and correction.<br />
Without it, they cannot truly understand.</p>

<blockquote>
  <p>“They learn what people would say, not what would happen,” Sutton notes.</p>
</blockquote>

<p>Put in another way, an LLM is a <strong>statistical mirror</strong> of human linguistic behavior. It reflects how people describe reality, not how reality unfolds.</p>

<p>But this raises a question: if LLMs truly lack a model that connects actions to consequences — the essence of understanding — then how do they still appear to know? When we ask LLM with a promot, “If I touch a boiling pot, what will happen?” the model will respond, “you’ll likely burn your skin immediately.” How should we reconcile this ability to state consequences with Sutton’s claim that LLMs don’t possess a world model? Or put more simply — is the ability to describe a consequence through language good enough, or even equivalent, to understanding the consequence itself?</p>

<h3 id="what-is-a-world-model">What Is a World Model?</h3>

<p>To unpack Sutton’s view, we must first understand what a <em>world model</em> is, why it is central to intelligence, and how it can be built.</p>

<p>Let’s begin with its simplest and most fundamental form: the dynamics model in reinforcement learning — a predictive model that describes how states evolve in response to actions.</p>

<p>A dynamics model is a function, often written as $\hat{f}$, that predicts the next state $s_{t+1}$ and reward $r_t$ given the current state $s_t$ and action $a_t$:</p>

\[s_{t+1},\ r_t \approx \hat{f}(s_t, a_t)\]

<p>With such a model, an <strong>agent</strong> can predict possible outcomes before acting — an ability fundamental to planning and reasoning.
In reality, we rarely have direct access to the true state of the world. Instead, we rely on observations from various senses — vision, hearing, touch — to infer an internal representation of the hidden state. State transition is also stochastic, reflecting ambiguity in how actions affect future states. A conceptual <strong>world model</strong> can therefore be represented as:</p>

\[\begin{aligned}
s_t &amp;= g(o_t) &amp;&amp; \text{(state inference)} \\
s_{t+1} &amp;\sim P(s_{t+1} \mid s_t, a_t) &amp;&amp; \text{(stochastic transition)} \\
r_t &amp;= r(s_t, a_t) &amp;&amp; \text{(reward model)} \\
o_t &amp;\sim P(o_t \mid s_t) &amp;&amp; \text{(observation generation)}
\end{aligned}\]

<h3 id="how-a-world-model-should-be-built">How a World Model Should Be Built</h3>

<p>Building a world model involves three intertwined processes:</p>

<ol>
  <li>
    <p><strong>Exploration</strong> — Gathering Experience</p>

    <p>To learn the dynamics of an environment, an agent must act within it. It explores different states and actions, collecting trajectories of experience:</p>

\[(s_t, a_t, r_t, s_{t+1})\]

    <p>These trajectories encode causal relationships — how actions lead to consequences — forming the empirical foundation of the model.</p>
  </li>
  <li>
    <p><strong>Perception</strong> — Interpreting Observations</p>

    <p>In most real-world settings, the true state $s_t$ is often hidden. Instead, the agent observes sensory inputs $o_t$ (images, sounds, text) and infer latent states:</p>

\[s_t \approx g(o_t)\]

    <p>Here, $g$ is a perceptual encoder that extracts task-relevant information from raw sensory data, transforming perception into internal understanding.</p>
  </li>
  <li>
    <p><strong>Learning</strong> - Modeling the Dynamics</p>

    <p>Finally, learning integrates exploration and perception into a coherent predictive framework. Given collected trajectories and inferred latent states, the agent learns a stochastic transition function that approximates how the world evolves:</p>

\[\hat{f}(s_t, a_t) \rightarrow (s_{t+1}, r_t)\]

    <p>This process typically involves representation learning (to encode  latent states), sequence modeling (to capture temporal dependencies), and probabilistic estimation (to handle ambiguity).</p>

    <p>With deep learning becoming the de facto approach for representation due to its expressive power, large neural networks now serve as the core mechanism for modeling dynamics. They are trained on sequences of state transitions as ground truth, where each step in the trajectory supervises the model to minimize prediction error between predicted and observed outcomes. Through this iterative process, the learned model captures both environmental regularities and stochastic variability — forming the foundation of world model learning.</p>
  </li>
</ol>

<h3 id="how-a-world-model-is-built-in-llms">How a World Model Is Built in LLMs</h3>

<p>Now, let’s turn to large language models (LLMs).
LLMs do not interact with the physical world directly. Instead, they learn from humans — observers and actors who have already explored the world and recorded their experiences as language. As shown in the diagram below, LLMs are trained on traces of human experience: vast text corpus that reflects how people perceive, reason about, and describe the world around them. Language serves as a powerful medium — distilling complex sensory, emotional, and causal interactions into symbols and sequences. Next-token prediction, the objective of pretraining, provides the concrete mechanism through which this learning occurs.</p>

<div class="mermaid">
graph TD
A[Human Interaction with the World] --&gt;|Trajectory| B[Experience Expressed in Language]
B --&gt;|Text Corpus| C[LLM Pretraining]
C --&gt;|Next-Token Prediction| D[LLM as Learned Model]
</div>

<h3 id="the-limitations-and-possible-paths-forward">The Limitations and Possible Paths Forward</h3>

<p>This process clearly introduces several fundamental limitations:</p>

<ul>
  <li>
    <p>Limited Exploration:
The exploration originates from human experience, producing a narrow trajectory that reflects only a fraction of the world. The model cannot act or gather new evidence beyond what humans have already recorded.</p>
  </li>
  <li>
    <p>Limited Perception and Representation:
LLMs perceive only text — a symbolic medium through which humans express sensory, visual, spatial, and auditory experiences. Inevitably, some information is lost or distorted in translation. This representational gap leads to incomplete or inaccurate estimates of the underlying state of the world.</p>
  </li>
</ul>

<p>Acknowledging these limitations, it is worth examining what learning truly means in this context — and whether next-token prediction provides a sufficient form of self-supervision and deep neural networks, serve as powerful function approximators, capable of capturing complex dependencies and representations at scale.</p>

<p>On this point, Ilya Sutskever — in his conversation with Dwarkesh Patel (same Youtube Podcast) <em><a href="https://www.youtube.com/watch?v=Yf1o0TQzry8">Why Next-Token Prediction Could Surpass Human Intelligence</a></em> — offered a particularly insightful perspective - Sutskever views next-token prediction as a profoundly deep task that extends far beyond surface-level statistics:</p>

<blockquote>
  <p>“Predicting the next token well means that you understand the underlying reality that led to the creation of that token.” While the process is statistical, Sutskever notes that the compression required to perform it effectively forces the model to internalize what it is about the world that produces those statistics.</p>
</blockquote>

<p>If this interpretation holds, then approaches such as increasing the diversity of pretraining data through large-scale simulation environments or expanding perception through multimodal models become practical and valuable directions for advancing LLMs.</p>

<p>Still, exploration remains the hardest frontier — models continue to rely on fixed set of human-collected trajectories. True world modeling requires interactive systems that can act, observe, and revise their own understanding through experience.</p>

<p><strong>Continuous learning</strong> represents a promising direction — adapting models within real-world environments through domain-specific fine-tuning. Such post-training adaptation could mark a step toward bridging static knowledge and experiential learning.
This also raises a compelling question: if we place an LLM in a new environment after pretraining, how well could it explore within that domain and generate new representations grounded in its evolving experience?</p>

<p>Viewed from another angle, we might ask whether a complete world model is even necessary.
Humans themselves operate under incomplete world models — constrained by limited perception and biased exploration — yet still act intelligently. Thus, the question shifts from “Can LLMs learn the whole world?” to “Can they reason, act, and gather enough information to accomplish the task at hand?”</p>

<p>That brings us to the next part: Goal and Policy — where understanding meets action.</p>]]></content><author><name></name></author><category term="LLM" /><category term="AI" /><category term="Deep Learning" /><category term="Reinforcement Learning" /><category term="LLM" /><category term="World Model" /><category term="Sutton" /></entry><entry><title type="html">Reflections on Richard Sutton’s Interview: Intro</title><link href="https://yuanxue.github.io/2025/10/05/reflection-sutton-intro.html" rel="alternate" type="text/html" title="Reflections on Richard Sutton’s Interview: Intro" /><published>2025-10-05T00:17:56-07:00</published><updated>2025-10-05T00:17:56-07:00</updated><id>https://yuanxue.github.io/2025/10/05/reflection-sutton-intro</id><content type="html" xml:base="https://yuanxue.github.io/2025/10/05/reflection-sutton-intro.html"><![CDATA[<p>I recently listened to <a href="https://www.youtube.com/watch?v=BF1aXbY0hS8">Richard Sutton’s interview on the <em>Dwarkesh Podcast</em></a>. Often called the father of reinforcement learning, Sutton spoke candidly about the limitations of large language models (LLMs), the importance of continual learning, and his lifelong conviction that true intelligence arises from experience—not imitation.</p>

<p>For me, the conversation was both intellectually dense and personally inspiring. I approach it from two perspectives:</p>

<ul>
  <li>
    <p>As a <strong>curious researcher</strong>, I’m drawn to questions at the core of AGI: What does it mean to understand and to act with purpose?</p>
  </li>
  <li>
    <p>As a <strong>practitioner</strong>, I would ask a more practical question:</p>
    <blockquote>
      <p><em>If our goal is not AGI, but AI systems that function as capable knowledge workers—delivering real-world, economically valuable tasks—is the current path of LLM scaling still a feasible one? And what boundaries or blindspots should we be mindful of?</em></p>
    </blockquote>
  </li>
</ul>

<p>These reflections led me to a set of questions:</p>

<ul>
  <li>Is the problem with next-token prediction itself?</li>
</ul>

<p>Next-token prediction defines pretraining and much of supervised fine-tuning. But modern post-training relies heavily on reinforcement learning—RLHF (human feedback) for human alignment and verifier-based RL (RLVF) for eliciting reasoning capabilities.
<em>Can post-training RL reshape a model’s behavior beyond pretraining limits—or does it remain bounded by what was initially imitated?</em></p>

<ul>
  <li>Is the limitation about when learning happens—training-time vs. deployment-time?</li>
</ul>

<p>Sutton argues for continual learning: adapting through direct experience after deployment.
Most current models are trained once and then frozen, accessed via APIs. However, this boundary is beginning to soften. Techniques like Reinforcement Fine-Tuning (RFT) allow models to adapt based on customer-specific objectives within real environments. Cursor’s recent work(https://cursor.com/en-US/blog/tab-rl) on online RL demonstrate early but promising signs of on-the-fly adaptation. If models can learn continuously from real-world interaction, could this represent a meaningful step toward addressing Sutton’s critique?</p>

<ul>
  <li>Is it about modality—text versus interaction-rich experience?</li>
</ul>

<p>LLMs originated in language, yet sparked a multimodal wave: diffusion models, Gemini, Claude 3, and embodied predictive models like Genie 3, which predicts future video frames from passive observation. Can multimodal prediction serve as a proxy for world experience—or is true interactivity necessary for agency?</p>

<ul>
  <li>If direct experience is essential, how do we scale it?</li>
</ul>

<p>The world is vast; no single agent can live enough lifetimes to experience it fully. Humans transcend this through language—abstracting and transmitting experiences across generations. If experience is the key, how do we scale it to match the richness of human understanding without millions of years of simulation?  And if <em>language abstraction</em> is not the answer, how can we enable scalable experience—and ensure that learned world models are passed on and inherited?</p>

<p>These questions echo ideas from many voices—Sutton’s belief in experience, Ilya Sutskever’s conviction in next-token prediction, and Andrej Karpathy’s optimism about scaling.</p>

<p>This blog series is my attempt to connect these threads: to explore where these views align, where they diverge, and how they might evolve. My hope is that by structuring the questions clearly, we can reason a little better—collectively.</p>]]></content><author><name></name></author><category term="LLM" /><category term="AI" /><category term="Deep Learning" /><category term="AGI" /><category term="Reinforcement Learning" /><category term="LLM" /></entry></feed>