<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://yuanxue.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://yuanxue.github.io/" rel="alternate" type="text/html" /><updated>2025-11-01T22:46:20-07:00</updated><id>https://yuanxue.github.io/feed.xml</id><title type="html">Yuan (Emily) Xue</title><subtitle>(last updated: Oct 2025)</subtitle><entry><title type="html">Reflections on Richard Sutton’s Interview: Part II — Goal and Acting</title><link href="https://yuanxue.github.io/2025/10/26/reflection-sutton-part2.html" rel="alternate" type="text/html" title="Reflections on Richard Sutton’s Interview: Part II — Goal and Acting" /><published>2025-10-26T00:00:00-07:00</published><updated>2025-10-26T00:00:00-07:00</updated><id>https://yuanxue.github.io/2025/10/26/reflection-sutton-part2</id><content type="html" xml:base="https://yuanxue.github.io/2025/10/26/reflection-sutton-part2.html"><![CDATA[<p><em>Understanding describes the world; goals decide what to do in it.</em></p>

<p>In <a href="https://yuanxue.github.io/2025/10/06/reflection-sutton-part1.html">Part I</a>, I discussed <em>world model</em> — what is it, why it is central to intelligence, how it can be built, how LLMs build such understanding, and where that understanding falls short.  In this second part, I turn to the other half of intelligence: <strong>acting toward a goal</strong>.</p>

<hr />

<h3 id="suttons-view-intelligence-requires-a-goal">Sutton’s View: Intelligence Requires a Goal</h3>

<p>As Richard Sutton reminds us of John McCarthy’s classic definition:</p>

<blockquote>
  <p>“Intelligence is the computational part of the ability to achieve goals.”
— John McCarthy</p>
</blockquote>

<p>Let’s begin again with the simplest form for expressing this objective. In reinforcement learning (RL), acting toward a goal is formalized through the <strong>Markov Decision Process (MDP)</strong>.</p>

<p>Recall that the world model (or dynamics model) describes how the environment changes and how rewards are generated based on the agent’s actions. This model provides the underlying components of the MDP:</p>

\[\begin{aligned}
s_t &amp;= g(o_t) &amp;&amp; \text{(state inference)} \\
s_{t+1} &amp;\sim P(s_{t+1} \mid s_t, a_t) &amp;&amp; \text{(stochastic transition)} \\
r_t &amp;= r(s_t, a_t) &amp;&amp; \text{(reward model)} \\
o_t &amp;\sim P(o_t \mid s_t) &amp;&amp; \text{(observation generation)}
\end{aligned}\]

<p>While the dynamics model tells us what will happen (the mechanics of the world),
the objective tells us what matters (the agent’s goal). We need a mathematical expression that formalizes this goal — one that captures intelligence as the ability to pursue desirable outcomes over time. This objective is formally defined as the expected cumulative discounted reward, or return:</p>

<p><strong>Objective</strong></p>

<p>This objective is defined as the <strong>expected cumulative discounted reward</strong>, or <em>return</em>:</p>

\[J = \mathbb{E}\!\left[\sum_t \gamma^t r_t\right]\]

<p>where:</p>

<ul>
  <li>$\mathbb{E}$ denotes the expected value of the reward sequence under the model’s behavior.</li>
  <li>$\gamma \in [0,1]$ is the <strong>discount factor</strong>, ensuring that rewards received sooner are valued more highly than those received later.</li>
  <li>$T$ is the <strong>time horizon</strong>, which may be finite or infinite.</li>
</ul>

<p><strong>Policy</strong></p>

<p>To act toward a goal, the agent needs a rule to decide <em>what to do</em> in order to optimize its reward. This rule is called the <strong>policy</strong>. Concretely, a policy $\pi(a\mid s)$ is a <strong>mapping from states to actions</strong> that dictates the agent’s behavior at every time step.  It defines <em>how</em> the agent behaves, given what it currently knows about the world.</p>

<p>The goal in reinforcement learning is to find the <strong>optimal policy</strong> — the one that maximizes the expected cumulative discounted reward (or <em>return</em>) over time.</p>

<p>In most cases, the policy is <strong>stochastic</strong>, meaning that the agent selects actions according to a probability distribution conditioned on the current state:</p>

\[a_t \sim \pi(a_t \mid s_t)\]

<p>This allows for exploration — the ability to sample different actions and discover better strategies over time.</p>

<p>Formally, the optimization objective can be expressed as:</p>

\[J(\pi) = \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{T} \gamma^t r_t \right]\]

<p>where $\tau = (s_0, a_0, s_1, a_1, \ldots)$ denotes a trajectory sampled according to the policy $\pi$.  The goal of learning is to find the policy $\pi^*$ that maximizes this expected return:</p>

\[\pi^* = \arg\max_{\pi} J(\pi)\]

<hr />

<h3 id="two-approaches-to-rl">Two Approaches to RL</h3>

<p>At a high level, reinforcement learning methods fall into two broad categories, depending on whether the agent has access to (or learns) a dynamics model. The diagram below (from <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html">OpenAI Spinning Up</a>) summarizes these families of RL algorithms:</p>

<p><img src="https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg" alt="RL Algorithms Overview" /></p>

<ul>
  <li>
    <p><strong>Model-based RL</strong>
Model-based methods use an explicit <em>world model</em> to simulate and plan future actions before execution. The key advantage is foresight — the ability to “think ahead,” evaluate the consequences of possible actions, and select the best one before acting.<br />
By using its model to plan, the agent can distill the results of this reasoning process into a learned policy.</p>

    <p>The main challenge is the availability of the dynamics (world) model. An imperfect model can mislead the agent into optimizing for its own errors — performing well within the learned model but failing in the real world.</p>
  </li>
  <li>
    <p><strong>Model-free RL</strong>
Model-free methods, by contrast, does not need an explicit <em>world model</em>, rather, it learns <em>directly from experience</em>. Instead of simulating possible futures, the agent interacts with the environment, receives rewards, and gradually adjusts its behavior based on observed outcomes.</p>

    <p>This family includes two major subcategories — <strong>value-based</strong> and <strong>policy-based</strong> approaches.</p>
  </li>
</ul>

<h4 id="value-based-approach">Value-Based Approach</h4>

<p>Value-based methods estimate the long-term return of each state or action using a <strong>value function</strong> $V(s)$ or an <strong>action-value function</strong> (or <strong>Q-function</strong>) $Q(s, a)$. These functions predict how good it is to be in a certain state or to take a certain action, in terms of future rewards. The value function is one of the <em>four fundamental components</em> of Sutton’s <em>“base common model”</em> of the RL agent.</p>

<p>This estimation connects short-term actions to long-term goals — a mechanism for learning from <em>sparse rewards</em> that occur far in the future. Sutton’s <em>startup analogy</em> captures this idea well: Imagine an entrepreneur working toward a long-term goal: the reward may arrive once in ten years — the “exit” moment when the startup succeeds. To stay motivated and effective, humans construct intermediate signals of progress: milestones, customer feedback, or product growth. Each small improvement updates the belief that the long-term goal is achievable, reinforcing the behaviors that contribute to it.</p>

<p>In RL, the value function serves this same purpose. When an agent achieves a local success it updates its estimate of eventual victory. That increase in expected value immediately reinforces the move that led to it.  This mechanism, known as <strong>temporal-difference (TD) learning</strong>, allows the agent to convert distant goals into immediate learning signals.</p>

<p>In essence, the <strong>value function</strong> transforms long-term objectives into tangible, auxiliary predictive rewards that drive learning in the moment.</p>

<h4 id="policy-based-approach">Policy-Based Approach</h4>

<p>The <strong>policy</strong> is another <em>fundamental component</em> of the RL agent based on Sutton’s <em>“base common model”</em> — the part that determines <em>how</em> it acts. Policy-based methods (also called <strong>policy optimization</strong>) take a different route from value-based ones. Rather than estimating value functions first, they directly <strong>optimize the policy parameters</strong> to improve expected performance.</p>

<p>A common technique, known as the <strong>policy gradient method</strong>, adjusts the policy parameters in proportion to the goodness of outcomes. Intuitively, actions that lead to higher returns are made more likely, while less successful actions are suppressed.</p>

<p>Formally, the policy gradient can be written as:</p>

\[\nabla_\theta J(\pi_\theta) = \mathbb{E}\!\left[\nabla_\theta \log \pi_\theta(a|s) \, Q^\pi(s,a)\right]\]

<p>This equation expresses a simple but profound idea: the gradient of performance depends on how likely an action is under the current policy and how valuable that action turns out to be. By following this gradient, the agent continually improves its behavior through experience.</p>

<p>Sutton summarized this intuitively in his interview:</p>

<blockquote>
  <p>“You act, you see what happens, and you change your behavior accordingly — not because someone told you what to do, but because the world responded.”</p>
</blockquote>

<p>This captures the essence of policy-based RL — <strong>intelligence as the art of adjustment to feedback</strong>.</p>

<hr />

<h3 id="llms-as-policies">LLMs as Policies</h3>

<p>Large Language Models (LLMs) can be viewed as <strong>policy models</strong> that map a given context (prompt) to a distribution over next-token actions. Sampling from the model corresponds to executing a stochastic policy $\pi_\theta(a_t \mid s_t)$, where:</p>

<ul>
  <li>$s_t$: the current input sequence,</li>
  <li>$a_t$: the token selected at time step $t$,</li>
  <li>$\theta$: the model parameters.</li>
</ul>

<p>And language generation can be expressed as a <strong>Markov Decision Process (MDP)</strong>, where</p>

<ul>
  <li>Transition is deterministic: $s_{t+1} = s_t \circ a_t$,</li>
  <li>Reward $r$ is a scalar value provided after sequence generation.</li>
</ul>

<p>To understand how this policy is learned — and how it differs from Sutton’s view — we must ask:  <strong>Does it have a goal? And if so, what is that goal?</strong>  Let’s look at how large language models are trained to seek an answer.</p>

<h3 id="from-behavior-cloning-to-reinforcement-learning">From Behavior Cloning to Reinforcement Learning</h3>

<p>LLMs undergo a multi-stage training process as <strong>Pre-Training</strong> and  <strong>Post-Training</strong>, which usually includes <strong>Supervised Fine-Tuning (SFT)</strong> and <strong>Reinforcement Learning</strong> with various types of rewards <em>Human Feedback (RLHF)</em>, <em>Verifiable Reward (RLVF)</em>, etc.</p>

<h4 id="pretraining-and-sft-policy-from-imitation"><em>Pretraining and SFT: Policy from Imitation</em></h4>

<p>In pretraining, the policy is optimized for next-token prediction from pretraining data — learning to imitate human linguistic behavior. This imitation-based paradigm effectively mirrors human language distributions rather than optimizing for outcomes. The model’s capability is limited by the quality and diversity of its training data. Without <em>goal-driven correction</em>, any flaws or biases in human data are inherited.</p>

<p>In SFT, the policy is trained directly on <strong>human demonstrations</strong>.  Given an input context $s_t$, the model learns to predict the next token $a_t$ that aligns with a human-provided reference output. Formally, the objective minimizes the negative log-likelihood of the target sequence:</p>

\[\mathcal{L}_{\text{SFT}} = - \sum_t \log \pi_\theta(a_t^{*} \mid s_t)\]

<p>Here, $a_t^{\ast}$ represents the human-annotated “correct” token at each step.<br />
Through this process, the model learns to imitate expert behavior — optimizing per-token likelihood given the context, rather than optimizing for task-specific rewards.  SFT refines the base model to follow human intent more reliably, but it remains fundamentally <strong>imitation-based</strong>. As a result, SFT has the following limitations:</p>

<ul>
  <li>Performance Ceiling: The model is constrained by the quality of its demonstrations. When the demonstrations are suboptimal, the resulting policy cannot surpass them and often inherits their flaws.</li>
  <li>Limited Generalization: Unable to adapt to unseen context. When the model deviates from the demonstrated trajectory and encounters out-of-distribution states, it can not recover.</li>
  <li>High Data Costs: Curating large-scale, high-quality human demonstration data is expensive and slow.</li>
</ul>

<h4 id="reinforcement-learning-in-post-training-policy-with-a-goal"><em>Reinforcement Learning in Post-Training: Policy with a Goal</em></h4>

<p>Post-training introduces <strong>explicit goals</strong> through reinforcement learning.</p>
<ul>
  <li>In <strong>RLHF (Reinforcement Learning from Human Feedback)</strong>, reward model is a learned neural network reflecting human preference.</li>
  <li>In <strong>RLVR</strong> (Verifiable Reward), rewards are derived from objective task success signals — such as solving a math problem or producing functionally correct code.</li>
  <li>In <strong>RLAIF</strong> (AI Feedback), the reward model is trained from preferences generated by a stronger or more aligned AI system rather than human annotators.</li>
</ul>

<p>These techniques transform LLMs from passive imitators into systems that optimize for defined reward/outcomes.</p>

<h3 id="open-reflections">Open Reflections</h3>

<h4 id="is-rl-in-post-training-enough">Is RL in Post-Training Enough?</h4>

<p>On the road to scaling RL, it’s pivotal to preserve exploration so the policy can keep finding novel trajectories rather than over-exploiting what already works. This is the classical exploration–exploitation dilemma in RL. For LLMs, this trade-off appears at every generation step: each token is an action sampled from the current model’s policy, so “exploration” is realized by sampling diversity. The diversity of this sampling is commonly quantified by <strong>policy entropy</strong>; higher entropy generally promotes broader exploration, while lower entropy tends to premature convergence.</p>

<p>In practice, RL for LLMs operates under two major constraints:</p>

<ul>
  <li>Policy-entropy decay. During RL fine-tuning of LLMs, policy entropy often declines over training—sometimes sharply if without explicit entropy or diversity controls. This makes the policy increasingly deterministic and reducing its ability to discover alternate paths. This reduction in exploration is frequently associated with performance plateaus.  Once the policy’s capacity for probabilistic exploration is exhausted, the model’s potential to improve through additional training or scaling becomes marginal.
<img src="https://arxiv.org/html/2505.22617v1/x2.png" alt="Entropy Mechanism in Reinforcement Learning for Reasoning LMs" /></li>
</ul>

<p><em>Figure source: “The entropy mechanism of reinforcement learning for reasoning language models.”</em> <a href="https://arxiv.org/abs/2505.22617"><em>arXiv:2505.22617</em></a>.</p>

<ul>
  <li>KL Divergence Regularization</li>
</ul>

<p>In the standard RLHF for LLMs, exploration is further constrained by a KL-divergence penalty. This penalty, $\beta \cdot \text{KL}(\pi_{\text{new}} | \pi_{\text{ref}})$, prevents the model ($\pi_{\text{new}}$) from drifting too far from its original distribution (typically the SFT or base LLM) ($\pi_{\text{ref}}$). This preserves language quality/fluency and prevents reward gaming, but it also constrains exploration by keeping the policy near its prior. KL-divergence can also be interpreted as a budget on policy deviation—a constraint that determines how far the model is allowed to move from its reference behavior during optimization. In this view, every training step “spends” part of this budget: increasing reward often comes at the cost of higher KL distance, meaning the model’s responses diverge more from its pre-trained tendencies. A small KL budget enforces conservative updates, maintaining linguistic fidelity but limiting potential reward gains. Conversely, a large KL budget allows greater policy flexibility and creative exploration, but risks linguistic degradation and reward hacking. Empirically, this trade-off forms a Pareto frontier between reward improvement and KL divergence, as shown in Scaling Laws for Reward Model Overoptimization (Gao, Schulman, Hilton, 2022). The x-axis represents KL divergence between the original and fine-tuned policies, while the y-axis shows the reward score. The curve demonstrates diminishing returns: beyond a certain KL threshold, additional divergence yields little reward gain and begins to degrade model alignment and quality.
<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/shcSdHGPhnLQkpSbX/hjb4tmendlzfrg7whc0j" alt="KL budget in RLHF" />
<em>Figure source: “Scaling Laws for Reward Model Overoptimization.”</em> <a href="https://arxiv.org/abs/2210.10760"><em>arXiv:2210.10760</em></a>.</p>

<p>In short: current post-training correction mechanisms are not enough. Exploration in RL-based fine-tuning for LLMs remains fundamentally insufficient—bounded by both policy entropy and KL regularization.</p>

<h4 id="new-dimension-of-exploration-via-test-time-compute">New Dimension of Exploration via Test-Time Compute</h4>

<p>While RLHF and RLVR primarily regulate exploration during training through entropy and KL constraints, reasoning models unlock a new form of exploration at inference time. Rather than modifying policy weights, they extend the model’s capacity to search and evaluate alternatives within a single forward pass—through thinking tokens, multi-step reasoning, or deliberation loops.</p>

<p>This “test-time compute” serves as an orthogonal axis of exploration. Here exploration shifts from the <em>policy space</em> to the <em>reasoning space</em>: instead of sampling <em>broader</em> actions across training trajectories, the model explores <em>deeper</em> reasoning trajectories within each input. This shift mirrors classical ideas in decision-making systems, such as Monte Carlo Tree Search, where extra compute is spent exploring possible outcomes under a fixed policy. The difference is that LLM reasoning performs this exploration implicitly, through self-generated text that <em>recursively conditions on its own prior thoughts</em>. In this way, test-time compute effectively decouples exploration from the KL constraint. The model can maintain alignment and language quality by staying close to its post-trained policy distribution, while still exploring richer internal reasoning pathways.</p>

<p>However, unlike RL exploration, which receives explicit reward signals, test-time reasoning operates in a self-supervised vacuum. The model “explores” but doesn’t learn from it — it can’t improve future reasoning efficiency or correctness unless test-time reasoning is integrated into a training loop.</p>

<p>In short: test-time compute reframes exploration as a dynamic allocation of inference resources rather than parameter change. Yet without learning feedback, this form of exploration remains transient — capable of deeper thought, but not of lasting improvement.</p>

<h3 id="closing-thought">Closing Thought</h3>

<p><strong>Can RL break the policy limits of pretraining? Partially.</strong></p>

<p>Sutton’s view reminds us that intelligence is the capacity to act toward a goal. RL formalizes this pursuit through policies that maximize the reward. When extended to LLMs, this framework reveals both promise and constraints: <em>pretraining and SFT teach imitation, while RLHF and RLVR introduce goal-directed correction but confines exploration within entropy and KL budgets</em>. Test-time compute expands the picture by <em>moving exploration from training to inference</em>. Yet this new dimension of exploration exposes another issue: intelligence can search more deeply, but without feedback, it cannot learn from its own thought.</p>

<p>Recent reasoning-RL models (e.g., OpenAI o1, DeepSeek-R1, Gemini 2.5) couple RL with reasoning. Empirical evidence demonstrates initial success, e.g. Gemini “Deep Think” system recently reached a gold-medal standard at the International Mathematical Olympiad. Yet whether this paradigam can break the constraints from pretrained linguistic and semantic priors, discover genuinely new reasoning strategies rather than performing a fuzzy form of deduction, or induce the kind of creative abstraction that defines general intelligence.</p>]]></content><author><name></name></author><category term="LLM" /><category term="AI" /><category term="Deep Learning" /><category term="Reinforcement Learning" /><category term="LLM" /><category term="AGI" /><category term="Sutton" /></entry><entry><title type="html">Reflections on Richard Sutton’s Interview: Part I — World Model and Understanding</title><link href="https://yuanxue.github.io/2025/10/06/reflection-sutton-part1.html" rel="alternate" type="text/html" title="Reflections on Richard Sutton’s Interview: Part I — World Model and Understanding" /><published>2025-10-06T00:00:00-07:00</published><updated>2025-10-06T00:00:00-07:00</updated><id>https://yuanxue.github.io/2025/10/06/reflection-sutton-part1</id><content type="html" xml:base="https://yuanxue.github.io/2025/10/06/reflection-sutton-part1.html"><![CDATA[<h3 id="suttons-view-lack-of-world-model-in-llms">Sutton’s View: Lack of World Model in LLMs</h3>

<p>Sutton begins with a simple observation: <strong>LLMs never experience the world.</strong> He argues that these systems <strong>lack a world model</strong> — a sense of cause and effect, surprise, and correction.<br />
Without it, they cannot truly understand.</p>

<blockquote>
  <p>“They learn what people would say, not what would happen,” Sutton notes.</p>
</blockquote>

<p>Put in another way, an LLM is a <strong>statistical mirror</strong> of human linguistic behavior. It reflects how people describe reality, not how reality unfolds.</p>

<p>But this raises a question: if LLMs truly lack a model that connects actions to consequences — the essence of understanding — then how do they still appear to know? When we ask LLM with a promot, “If I touch a boiling pot, what will happen?” the model will respond, “you’ll likely burn your skin immediately.” How should we reconcile this ability to state consequences with Sutton’s claim that LLMs don’t possess a world model? Or put more simply — is the ability to describe a consequence through language good enough, or even equivalent, to understanding the consequence itself?</p>

<h3 id="what-is-a-world-model">What Is a World Model?</h3>

<p>To unpack Sutton’s view, we must first understand what a <em>world model</em> is, why it is central to intelligence, and how it can be built.</p>

<p>Let’s begin with its simplest and most fundamental form: the dynamics model in reinforcement learning — a predictive model that describes how states evolve in response to actions.</p>

<p>A dynamics model is a function, often written as $\hat{f}$, that predicts the next state $s_{t+1}$ and reward $r_t$ given the current state $s_t$ and action $a_t$:</p>

\[s_{t+1},\ r_t \approx \hat{f}(s_t, a_t)\]

<p>With such a model, an <strong>agent</strong> can predict possible outcomes before acting — an ability fundamental to planning and reasoning.
In reality, we rarely have direct access to the true state of the world. Instead, we rely on observations from various senses — vision, hearing, touch — to infer an internal representation of the hidden state. State transition is also stochastic, reflecting ambiguity in how actions affect future states. A conceptual <strong>world model</strong> can therefore be represented as:</p>

\[\begin{aligned}
s_t &amp;= g(o_t) &amp;&amp; \text{(state inference)} \\
s_{t+1} &amp;\sim P(s_{t+1} \mid s_t, a_t) &amp;&amp; \text{(stochastic transition)} \\
r_t &amp;= r(s_t, a_t) &amp;&amp; \text{(reward model)} \\
o_t &amp;\sim P(o_t \mid s_t) &amp;&amp; \text{(observation generation)}
\end{aligned}\]

<h3 id="how-a-world-model-should-be-built">How a World Model Should Be Built</h3>

<p>Building a world model involves three intertwined processes:</p>

<ol>
  <li>
    <p><strong>Exploration</strong> — Gathering Experience</p>

    <p>To learn the dynamics of an environment, an agent must act within it. It explores different states and actions, collecting trajectories of experience:</p>

\[(s_t, a_t, r_t, s_{t+1})\]

    <p>These trajectories encode causal relationships — how actions lead to consequences — forming the empirical foundation of the model.</p>
  </li>
  <li>
    <p><strong>Perception</strong> — Interpreting Observations</p>

    <p>In most real-world settings, the true state $s_t$ is often hidden. Instead, the agent observes sensory inputs $o_t$ (images, sounds, text) and infer latent states:</p>

\[s_t \approx g(o_t)\]

    <p>Here, $g$ is a perceptual encoder that extracts task-relevant information from raw sensory data, transforming perception into internal understanding.</p>
  </li>
  <li>
    <p><strong>Learning</strong> - Modeling the Dynamics</p>

    <p>Finally, learning integrates exploration and perception into a coherent predictive framework. Given collected trajectories and inferred latent states, the agent learns a stochastic transition function that approximates how the world evolves:</p>

\[\hat{f}(s_t, a_t) \rightarrow (s_{t+1}, r_t)\]

    <p>This process typically involves representation learning (to encode  latent states), sequence modeling (to capture temporal dependencies), and probabilistic estimation (to handle ambiguity).</p>

    <p>With deep learning becoming the de facto approach for representation due to its expressive power, large neural networks now serve as the core mechanism for modeling dynamics. They are trained on sequences of state transitions as ground truth, where each step in the trajectory supervises the model to minimize prediction error between predicted and observed outcomes. Through this iterative process, the learned model captures both environmental regularities and stochastic variability — forming the foundation of world model learning.</p>
  </li>
</ol>

<h3 id="how-a-world-model-is-built-in-llms">How a World Model Is Built in LLMs</h3>

<p>Now, let’s turn to large language models (LLMs).
LLMs do not interact with the physical world directly. Instead, they learn from humans — observers and actors who have already explored the world and recorded their experiences as language. As shown in the diagram below, LLMs are trained on traces of human experience: vast text corpus that reflects how people perceive, reason about, and describe the world around them. Language serves as a powerful medium — distilling complex sensory, emotional, and causal interactions into symbols and sequences. Next-token prediction, the objective of pretraining, provides the concrete mechanism through which this learning occurs.</p>

<div class="mermaid">
graph TD
A[Human Interaction with the World] --&gt;|Trajectory| B[Experience Expressed in Language]
B --&gt;|Text Corpus| C[LLM Pretraining]
C --&gt;|Next-Token Prediction| D[LLM as Learned Model]
</div>

<h3 id="the-limitations-and-possible-paths-forward">The Limitations and Possible Paths Forward</h3>

<p>This process clearly introduces several fundamental limitations:</p>

<ul>
  <li>
    <p>Limited Exploration:
The exploration originates from human experience, producing a narrow trajectory that reflects only a fraction of the world. The model cannot act or gather new evidence beyond what humans have already recorded.</p>
  </li>
  <li>
    <p>Limited Perception and Representation:
LLMs perceive only text — a symbolic medium through which humans express sensory, visual, spatial, and auditory experiences. Inevitably, some information is lost or distorted in translation. This representational gap leads to incomplete or inaccurate estimates of the underlying state of the world.</p>
  </li>
</ul>

<p>Acknowledging these limitations, it is worth examining what learning truly means in this context — and whether next-token prediction provides a sufficient form of self-supervision and deep neural networks, serve as powerful function approximators, capable of capturing complex dependencies and representations at scale.</p>

<p>On this point, Ilya Sutskever — in his conversation with Dwarkesh Patel (same Youtube Podcast) <em><a href="https://www.youtube.com/watch?v=Yf1o0TQzry8">Why Next-Token Prediction Could Surpass Human Intelligence</a></em> — offered a particularly insightful perspective - Sutskever views next-token prediction as a profoundly deep task that extends far beyond surface-level statistics:</p>

<blockquote>
  <p>“Predicting the next token well means that you understand the underlying reality that led to the creation of that token.” While the process is statistical, Sutskever notes that the compression required to perform it effectively forces the model to internalize what it is about the world that produces those statistics.</p>
</blockquote>

<p>If this interpretation holds, then approaches such as increasing the diversity of pretraining data through large-scale simulation environments or expanding perception through multimodal models become practical and valuable directions for advancing LLMs.</p>

<p>Still, exploration remains the hardest frontier — models continue to rely on fixed set of human-collected trajectories. True world modeling requires interactive systems that can act, observe, and revise their own understanding through experience.</p>

<p><strong>Continuous learning</strong> represents a promising direction — adapting models within real-world environments through domain-specific fine-tuning. Such post-training adaptation could mark a step toward bridging static knowledge and experiential learning.
This also raises a compelling question: if we place an LLM in a new environment after pretraining, how well could it explore within that domain and generate new representations grounded in its evolving experience?</p>

<p>Viewed from another angle, we might ask whether a complete world model is even necessary.
Humans themselves operate under incomplete world models — constrained by limited perception and biased exploration — yet still act intelligently. Thus, the question shifts from “Can LLMs learn the whole world?” to “Can they reason, act, and gather enough information to accomplish the task at hand?”</p>

<p>That brings us to the next part: Goal and Policy — where understanding meets action.</p>]]></content><author><name></name></author><category term="LLM" /><category term="AI" /><category term="Deep Learning" /><category term="Reinforcement Learning" /><category term="LLM" /><category term="World Model" /><category term="Sutton" /></entry><entry><title type="html">Reflections on Richard Sutton’s Interview: Intro</title><link href="https://yuanxue.github.io/2025/10/05/reflection-sutton-intro.html" rel="alternate" type="text/html" title="Reflections on Richard Sutton’s Interview: Intro" /><published>2025-10-05T00:17:56-07:00</published><updated>2025-10-05T00:17:56-07:00</updated><id>https://yuanxue.github.io/2025/10/05/reflection-sutton-intro</id><content type="html" xml:base="https://yuanxue.github.io/2025/10/05/reflection-sutton-intro.html"><![CDATA[<p>I recently listened to <a href="https://www.youtube.com/watch?v=BF1aXbY0hS8">Richard Sutton’s interview on the <em>Dwarkesh Podcast</em></a>. Often called the father of reinforcement learning, Sutton spoke candidly about the limitations of large language models (LLMs), the importance of continual learning, and his lifelong conviction that true intelligence arises from experience—not imitation.</p>

<p>For me, the conversation was both intellectually dense and personally inspiring. I approach it from two perspectives:</p>

<ul>
  <li>
    <p>As a <strong>curious researcher</strong>, I’m drawn to questions at the core of AGI: What does it mean to understand and to act with purpose?</p>
  </li>
  <li>
    <p>As a <strong>practitioner</strong>, I would ask a more practical question:</p>
    <blockquote>
      <p><em>If our goal is not AGI, but AI systems that function as capable knowledge workers—delivering real-world, economically valuable tasks—is the current path of LLM scaling still a feasible one? And what boundaries or blindspots should we be mindful of?</em></p>
    </blockquote>
  </li>
</ul>

<p>These reflections led me to a set of questions:</p>

<ul>
  <li>Is the problem with next-token prediction itself?</li>
</ul>

<p>Next-token prediction defines pretraining and much of supervised fine-tuning. But modern post-training relies heavily on reinforcement learning—RLHF (human feedback) for human alignment and verifier-based RL (RLVF) for eliciting reasoning capabilities.
<em>Can post-training RL reshape a model’s behavior beyond pretraining limits—or does it remain bounded by what was initially imitated?</em></p>

<ul>
  <li>Is the limitation about when learning happens—training-time vs. deployment-time?</li>
</ul>

<p>Sutton argues for continual learning: adapting through direct experience after deployment.
Most current models are trained once and then frozen, accessed via APIs. However, this boundary is beginning to soften. Techniques like Reinforcement Fine-Tuning (RFT) allow models to adapt based on customer-specific objectives within real environments. Cursor’s recent work(https://cursor.com/en-US/blog/tab-rl) on online RL demonstrate early but promising signs of on-the-fly adaptation. If models can learn continuously from real-world interaction, could this represent a meaningful step toward addressing Sutton’s critique?</p>

<ul>
  <li>Is it about modality—text versus interaction-rich experience?</li>
</ul>

<p>LLMs originated in language, yet sparked a multimodal wave: diffusion models, Gemini, Claude 3, and embodied predictive models like Genie 3, which predicts future video frames from passive observation. Can multimodal prediction serve as a proxy for world experience—or is true interactivity necessary for agency?</p>

<ul>
  <li>If direct experience is essential, how do we scale it?</li>
</ul>

<p>The world is vast; no single agent can live enough lifetimes to experience it fully. Humans transcend this through language—abstracting and transmitting experiences across generations. If experience is the key, how do we scale it to match the richness of human understanding without millions of years of simulation?  And if <em>language abstraction</em> is not the answer, how can we enable scalable experience—and ensure that learned world models are passed on and inherited?</p>

<p>These questions echo ideas from many voices—Sutton’s belief in experience, Ilya Sutskever’s conviction in next-token prediction, and Andrej Karpathy’s optimism about scaling.</p>

<p>This blog series is my attempt to connect these threads: to explore where these views align, where they diverge, and how they might evolve. My hope is that by structuring the questions clearly, we can reason a little better—collectively.</p>]]></content><author><name></name></author><category term="LLM" /><category term="AI" /><category term="Deep Learning" /><category term="AGI" /><category term="Reinforcement Learning" /><category term="LLM" /></entry></feed>