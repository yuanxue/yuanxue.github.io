<!DOCTYPE html>
<html lang="en">
  <head><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Reflections on Richard Sutton’s Interview: Part II — Goal and Acting | Yuan (Emily) Xue</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Reflections on Richard Sutton’s Interview: Part II — Goal and Acting" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Check out my blog." />
<meta property="og:description" content="Check out my blog." />
<link rel="canonical" href="https://yuanxue.github.io/2025/10/07/reflection-sutton-part2.html" />
<meta property="og:url" content="https://yuanxue.github.io/2025/10/07/reflection-sutton-part2.html" />
<meta property="og:site_name" content="Yuan (Emily) Xue" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-10-07T00:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Reflections on Richard Sutton’s Interview: Part II — Goal and Acting" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-10-07T00:00:00-07:00","datePublished":"2025-10-07T00:00:00-07:00","description":"Check out my blog.","headline":"Reflections on Richard Sutton’s Interview: Part II — Goal and Acting","mainEntityOfPage":{"@type":"WebPage","@id":"https://yuanxue.github.io/2025/10/07/reflection-sutton-part2.html"},"url":"https://yuanxue.github.io/2025/10/07/reflection-sutton-part2.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://yuanxue.github.io/feed.xml" title="Yuan (Emily) Xue" /></head>
</head>
  <body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Yuan (Emily) Xue</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/">Blog</a><a class="page-link" href="/">Home</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Reflections on Richard Sutton&#39;s Interview: Part II — Goal and Acting</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2025-10-07T00:00:00-07:00" itemprop="datePublished">Oct 7, 2025
      </time></p>
      <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']]
        },
        svg: {
          fontCache: 'global'
        }
      };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
  </header>

    <nav class="post-toc" aria-label="Table of Contents">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h3"><a href="#intelligence-requires-a-goal">Intelligence Requires a Goal</a></li>
<li class="toc-entry toc-h3"><a href="#from-dynamics-to-objective">From Dynamics to Objective</a></li>
<li class="toc-entry toc-h3"><a href="#two-broad-approaches-to-rl">Two Broad Approaches to RL</a>
<ul>
<li class="toc-entry toc-h4"><a href="#value-based-approach">Value-Based Approach</a></li>
<li class="toc-entry toc-h4"><a href="#policy-based-approach">Policy-Based Approach</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#the-llm-analogy-policy-without-and-with-goals">The LLM Analogy: Policy Without and With Goals</a>
<ul>
<li class="toc-entry toc-h4"><a href="#pretraining-policy-from-imitation">Pretraining: Policy from Imitation</a></li>
<li class="toc-entry toc-h4"><a href="#post-training-policy-with-a-goal">Post-Training: Policy with a Goal</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#open-reflections">Open Reflections</a>
<ul>
<li class="toc-entry toc-h4"><a href="#is-post-training-correction-enough">Is Post-Training Correction Enough?</a></li>
<li class="toc-entry toc-h4"><a href="#reasoning-as-exploration">Reasoning as Exploration</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#closing-thought">Closing Thought</a></li>
</ul>
  </nav>

  <div class="post-content e-content" itemprop="articleBody">
    
<p>In <a href="#">Part I</a>, I explored the idea of a <em>world model</em> — how systems like large language models (LLMs) understand the world, and where that understanding falls short.<br />
In this second part, I turn to the other half of intelligence: <strong>acting toward a goal</strong>.<br />
Understanding describes the world; goals decide what to do in it.</p>

<hr />

<h3 id="intelligence-requires-a-goal">Intelligence Requires a Goal</h3>

<p>As Richard Sutton reminds us, <em>“Intelligence is about achieving goals in the world.”</em><br />
A system can model the world endlessly, but without a purpose, its knowledge remains inert.<br />
Where Part I focused on learning the world’s dynamics, this part centers on <strong>learning to act</strong> — the bridge from prediction to decision.</p>

<p>In reinforcement learning (RL), this bridge is formalized through the <strong>Markov Decision Process (MDP)</strong>, defined by a tuple ( (S, A, P, R, \gamma) ):</p>
<ul>
  <li>( S ): states</li>
  <li>( A ): actions</li>
  <li>( P ): transition dynamics (the world model)</li>
  <li>( R ): reward function</li>
  <li>( \gamma ): discount factor for future rewards</li>
</ul>

<p>The world model tells us <em>what happens</em>; the reward tells us <em>what matters</em>.</p>

<hr />

<h3 id="from-dynamics-to-objective">From Dynamics to Objective</h3>

<p>Once we can model transitions ( f(s, a) \rightarrow (s’, r) ), we can define the agent’s objective:</p>

<p>[
J(\pi) = \mathbb{E}_{\pi}!\left[\sum_t \gamma^t r_t\right]
]</p>

<p>This objective expresses intelligence as <em>optimization through experience</em>.<br />
The agent learns a <strong>policy</strong> ( \pi(a|s) ) — a mapping from states to actions — that maximizes expected cumulative reward.</p>

<p>Two broad approaches emerge:</p>
<ul>
  <li><strong>Model-based RL</strong>, which plans actions using an explicit world model.</li>
  <li><strong>Model-free RL</strong>, which learns directly from trial and error, without ever constructing the model explicitly.</li>
</ul>

<hr />

<h3 id="two-broad-approaches-to-rl">Two Broad Approaches to RL</h3>

<h4 id="value-based-approach"><strong>Value-Based Approach</strong></h4>

<p>Value-based methods estimate the long-term return of each state or action via a <strong>value function</strong> ( V(s) ) or <strong>action-value function</strong> ( Q(s, a) ).<br />
This estimation connects short-term decisions to long-term outcomes — a way to see beyond the immediate reward.<br />
Sutton’s <em>startup analogy</em> illustrates this: an early-stage company may take actions that lose money now, but yield value later.<br />
Without a sense of long-term value, intelligent action collapses into short-sighted imitation.</p>

<h4 id="policy-based-approach"><strong>Policy-Based Approach</strong></h4>

<p>Policy-based methods take a different route. Instead of estimating value first, they directly <strong>optimize the policy parameters</strong> to improve expected performance.<br />
This approach, known as <strong>policy gradient</strong>, adjusts the policy in proportion to the goodness of outcomes:</p>

<p>[
\nabla_\theta J(\pi_\theta) = \mathbb{E}!\left[\nabla_\theta \log \pi_\theta(a|s) \, Q^\pi(s,a)\right]
]</p>

<p>In Sutton’s interview, he put it simply:</p>
<blockquote>
  <p>“You act, you see what happens, and you change your behavior accordingly — not because someone told you what to do, but because the world responded.”</p>
</blockquote>

<p>This line captures the spirit of policy-based RL: intelligence as the art of adjusting to feedback, not imitating prescriptions.</p>

<hr />

<h3 id="the-llm-analogy-policy-without-and-with-goals">The LLM Analogy: Policy Without and With Goals</h3>

<h4 id="pretraining-policy-from-imitation"><strong>Pretraining: Policy from Imitation</strong></h4>

<p>A pretrained LLM can be viewed as a <strong>policy</strong> ( \pi_\text{LLM}(a_t | s_t) ) that imitates human linguistic behavior.<br />
It generates the next token by predicting what <em>a human would likely say next</em>.<br />
This is statistical learning — powerful but passive.<br />
Such a model lacks goal-driven correction; it learns human <em>descriptions</em> of success, not the <em>experience</em> of success itself.</p>

<h4 id="post-training-policy-with-a-goal"><strong>Post-Training: Policy with a Goal</strong></h4>

<p>Post-training introduces goals through <strong>reinforcement learning</strong>.<br />
In <strong>RLHF (Reinforcement Learning from Human Feedback)</strong>, the goal is to align outputs with human preferences encoded in a reward model.<br />
In <strong>RLVR</strong> and related techniques, the objective becomes <em>task success</em> — e.g., solving a math problem or generating correct code.</p>

<p>These phases transform LLMs from pure imitators into systems that optimize for explicit outcomes.<br />
Yet the transformation is partial: the model learns within narrow, human-specified boundaries.<br />
It is still far from an open-ended learner that explores and redefines its goals through experience.</p>

<hr />

<h3 id="open-reflections">Open Reflections</h3>

<h4 id="is-post-training-correction-enough"><strong>Is Post-Training Correction Enough?</strong></h4>

<p>RLHF and RLVR bring purpose, but within limited and static objectives.<br />
Recent work in RL emphasizes the importance of <strong>entropy</strong> — the capacity to keep exploring.<br />
High-entropy policies resist premature convergence, maintaining curiosity in uncertain environments.<br />
Perhaps the next leap for LLMs lies in <em>continual, on-policy interaction</em> with the world — learning not just from labeled feedback but from the consequences of their own actions.</p>

<h4 id="reasoning-as-exploration"><strong>Reasoning as Exploration</strong></h4>

<p>Reasoning models and <strong>Chain-of-Thought (CoT)</strong> prompting introduce implicit exploration.<br />
Each reasoning step expands the model’s internal trajectory beyond the training corpus, forming a kind of <em>mental simulation</em>.<br />
Inference-time scaling — generating multiple thought chains before selecting the best one — resembles <em>policy rollout and evaluation</em>.<br />
Could this process act as an internal feedback loop, a primitive form of on-policy learning?</p>

<hr />

<h3 id="closing-thought">Closing Thought</h3>

<p>Understanding is half of intelligence; acting completes it.<br />
A world model provides <em>prediction</em>, but a goal gives <em>direction</em>.<br />
As Sutton has long argued, the essence of intelligence lies in the continual cycle of <strong>acting, observing, and adjusting</strong>.<br />
Only when knowledge and purpose form a closed loop can we say a system truly learns from the world — not just about it.</p>

  </div><a class="u-url" href="/2025/10/07/reflection-sutton-part2.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Yuan (Emily) Xue</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Yuan (Emily) Xue</li><li><a class="u-email" href="mailto:yuanxue00@gmail.com">yuanxue00@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/yuanxue"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">yuanxue</span></a></li><li><a href="https://www.twitter.com/yuanxue00"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">yuanxue00</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Check out my blog. </p>
      </div>
    </div>

  </div>

</footer>

      <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
      <script>
        document.addEventListener('DOMContentLoaded', function () {
          mermaid.initialize({
            startOnLoad: true,
            securityLevel: 'loose' // allows <br/> in labels
          });
        });
      </script>
    
  </body>
</html>
